{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3777652d",
   "metadata": {},
   "source": [
    "# Notebook 02: Data Cleaning & Preprocessing\n",
    "\n",
    "This notebook prepares the dataset for modeling. We perform data cleaning, encoding, scaling, and train-test splitting.\n",
    "\n",
    "### Objectives of This Notebook:\n",
    "- Drop irrelevant or redundant columns\n",
    "- Encode categorical and target variables\n",
    "- Scale numerical features\n",
    "- Split the dataset into training and testing sets\n",
    "- Save cleaned files for future modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cafd16",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "We import the necessary preprocessing tools from `sklearn`, along with `pandas` for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c94e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61341823",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "We load the same raw dataset used in Notebook 01. Cleaning will be performed on this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feea92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw_data.csv\")\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209a729",
   "metadata": {},
   "source": [
    "### Step 3: Drop Irrelevant Columns\n",
    "We remove columns that won't help with predictions (e.g., identifiers and pre-built model predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef3b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\n",
    "    \"CLIENTNUM\",\n",
    "    \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n",
    "    \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4af47",
   "metadata": {},
   "source": [
    "### Step 4: Encode the Target Column\n",
    "We convert the target label to a binary integer (0 = existing, 1 = churned). This is required for most ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc734161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Attrition_Flag\"] = df[\"Attrition_Flag\"].apply(lambda x: 1 if x == \"Attrited Customer\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292936f",
   "metadata": {},
   "source": [
    "### Step 5: Encode Categorical Features\n",
    "Label encoding is used to convert categorical features into numeric codes. This allows models to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47cce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83878129",
   "metadata": {},
   "source": [
    "### Step 6: Train-Test Split\n",
    "We separate the data into features (X) and target (y), then split into training and test sets with stratification to preserve churn ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Attrition_Flag\", axis=1)\n",
    "y = df[\"Attrition_Flag\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e33010",
   "metadata": {},
   "source": [
    "### Step 7: Feature Scaling\n",
    "We scale only the numeric columns to standardize their ranges. This benefits many models like Logistic Regression and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb32f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881032d",
   "metadata": {},
   "source": [
    "### Step 8: Preview Cleaned Datasets\n",
    "Let’s preview the transformed features and labels to ensure everything looks clean and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b49d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Card_Category</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>-1.301232</td>\n",
       "      <td>-0.946732</td>\n",
       "      <td>-1.809900</td>\n",
       "      <td>1.58682</td>\n",
       "      <td>-0.627024</td>\n",
       "      <td>0.760924</td>\n",
       "      <td>-0.256393</td>\n",
       "      <td>-1.513205</td>\n",
       "      <td>0.122196</td>\n",
       "      <td>0.646495</td>\n",
       "      <td>-0.411236</td>\n",
       "      <td>-0.665434</td>\n",
       "      <td>1.303942</td>\n",
       "      <td>-0.783057</td>\n",
       "      <td>-0.335597</td>\n",
       "      <td>-0.775093</td>\n",
       "      <td>-0.961644</td>\n",
       "      <td>-1.093573</td>\n",
       "      <td>2.148867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>-0.299340</td>\n",
       "      <td>1.056265</td>\n",
       "      <td>0.501503</td>\n",
       "      <td>-0.05625</td>\n",
       "      <td>0.726432</td>\n",
       "      <td>-0.565148</td>\n",
       "      <td>4.128624</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.765411</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>0.489635</td>\n",
       "      <td>1.848571</td>\n",
       "      <td>-1.433246</td>\n",
       "      <td>1.977862</td>\n",
       "      <td>0.407489</td>\n",
       "      <td>-0.619960</td>\n",
       "      <td>-1.131083</td>\n",
       "      <td>-0.537244</td>\n",
       "      <td>-1.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7141</th>\n",
       "      <td>-0.048867</td>\n",
       "      <td>1.056265</td>\n",
       "      <td>-0.268965</td>\n",
       "      <td>-0.05625</td>\n",
       "      <td>-0.627024</td>\n",
       "      <td>0.097888</td>\n",
       "      <td>-0.256393</td>\n",
       "      <td>-0.756952</td>\n",
       "      <td>1.408625</td>\n",
       "      <td>0.646495</td>\n",
       "      <td>-1.312106</td>\n",
       "      <td>0.342117</td>\n",
       "      <td>-0.310139</td>\n",
       "      <td>0.370094</td>\n",
       "      <td>0.603518</td>\n",
       "      <td>-0.032694</td>\n",
       "      <td>1.029259</td>\n",
       "      <td>0.579534</td>\n",
       "      <td>-0.717103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>-1.301232</td>\n",
       "      <td>-0.946732</td>\n",
       "      <td>-0.268965</td>\n",
       "      <td>-0.60394</td>\n",
       "      <td>-0.627024</td>\n",
       "      <td>0.760924</td>\n",
       "      <td>-0.256393</td>\n",
       "      <td>-1.513205</td>\n",
       "      <td>-0.521019</td>\n",
       "      <td>-1.331320</td>\n",
       "      <td>0.489635</td>\n",
       "      <td>-0.604095</td>\n",
       "      <td>0.522064</td>\n",
       "      <td>-0.651190</td>\n",
       "      <td>0.498665</td>\n",
       "      <td>-0.805413</td>\n",
       "      <td>-1.004004</td>\n",
       "      <td>-1.427370</td>\n",
       "      <td>0.850111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0.452079</td>\n",
       "      <td>1.056265</td>\n",
       "      <td>-1.039432</td>\n",
       "      <td>-0.60394</td>\n",
       "      <td>0.726432</td>\n",
       "      <td>-1.891219</td>\n",
       "      <td>-0.256393</td>\n",
       "      <td>0.503468</td>\n",
       "      <td>0.122196</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>0.489635</td>\n",
       "      <td>2.871622</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>2.869713</td>\n",
       "      <td>-0.157803</td>\n",
       "      <td>-0.151325</td>\n",
       "      <td>0.309145</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>-0.876727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Customer_Age    Gender  Dependent_count  Education_Level  \\\n",
       "2856     -1.301232 -0.946732        -1.809900          1.58682   \n",
       "6515     -0.299340  1.056265         0.501503         -0.05625   \n",
       "7141     -0.048867  1.056265        -0.268965         -0.05625   \n",
       "632      -1.301232 -0.946732        -0.268965         -0.60394   \n",
       "3496      0.452079  1.056265        -1.039432         -0.60394   \n",
       "\n",
       "      Marital_Status  Income_Category  Card_Category  Months_on_book  \\\n",
       "2856       -0.627024         0.760924      -0.256393       -1.513205   \n",
       "6515        0.726432        -0.565148       4.128624       -0.000700   \n",
       "7141       -0.627024         0.097888      -0.256393       -0.756952   \n",
       "632        -0.627024         0.760924      -0.256393       -1.513205   \n",
       "3496        0.726432        -1.891219      -0.256393        0.503468   \n",
       "\n",
       "      Total_Relationship_Count  Months_Inactive_12_mon  Contacts_Count_12_mon  \\\n",
       "2856                  0.122196                0.646495              -0.411236   \n",
       "6515                  0.765411               -0.342413               0.489635   \n",
       "7141                  1.408625                0.646495              -1.312106   \n",
       "632                  -0.521019               -1.331320               0.489635   \n",
       "3496                  0.122196               -0.342413               0.489635   \n",
       "\n",
       "      Credit_Limit  Total_Revolving_Bal  Avg_Open_To_Buy  \\\n",
       "2856     -0.665434             1.303942        -0.783057   \n",
       "6515      1.848571            -1.433246         1.977862   \n",
       "7141      0.342117            -0.310139         0.370094   \n",
       "632      -0.604095             0.522064        -0.651190   \n",
       "3496      2.871622             0.021269         2.869713   \n",
       "\n",
       "      Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  Total_Trans_Ct  \\\n",
       "2856             -0.335597        -0.775093       -0.961644   \n",
       "6515              0.407489        -0.619960       -1.131083   \n",
       "7141              0.603518        -0.032694        1.029259   \n",
       "632               0.498665        -0.805413       -1.004004   \n",
       "3496             -0.157803        -0.151325        0.309145   \n",
       "\n",
       "      Total_Ct_Chng_Q4_Q1  Avg_Utilization_Ratio  \n",
       "2856            -1.093573               2.148867  \n",
       "6515            -0.537244              -1.000072  \n",
       "7141             0.579534              -0.717103  \n",
       "632             -1.427370               0.850111  \n",
       "3496             0.002601              -0.876727  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train distribution:\n",
      "Attrition_Flag\n",
      "0    6799\n",
      "1    1302\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train preview:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\ny_train distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848d536",
   "metadata": {},
   "source": [
    "The cleaned training dataset is now ready for modeling:\n",
    "- `X_train` shows all features have been numerically encoded and scaled, ensuring compatibility with most ML models.\n",
    "- `y_train` class counts:\n",
    "  - **0 (Existing Customers): 6799**\n",
    "  - **1 (Attrited Customers): 1302**\n",
    "  - This confirms we still have an imbalanced target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820f25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_test distribution:\n",
      "Attrition_Flag\n",
      "0    1701\n",
      "1     325\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ny_test distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772bb3c",
   "metadata": {},
   "source": [
    "**Stratification Check:**  \n",
    "The `y_test` distribution is as follows:\n",
    "- **0 (Existing Customers): 1701**\n",
    "- **1 (Attrited Customers): 325**\n",
    "\n",
    "This closely mirrors the training set proportions, confirming that **stratified splitting worked as intended**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc8285",
   "metadata": {},
   "source": [
    "### Step 9: Save Preprocessed Data\n",
    "We export the cleaned datasets as CSVs for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eff9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../data/X_train_clean.csv\", index=False)\n",
    "X_test.to_csv(\"../data/X_test_clean.csv\", index=False)\n",
    "y_train.to_csv(\"../data/y_train_clean.csv\", index=False)\n",
    "y_test.to_csv(\"../data/y_test_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b667d5",
   "metadata": {},
   "source": [
    "### Step 10: Summary & Next Steps\n",
    "We now have CSVs that are cleaned and ready for machine learning algorithms. \n",
    "\n",
    "In Notebook 03, we’ll: \n",
    "\n",
    "- Begin building predictive models to classify churn risk, starting with a baseline Logistic Regression model. \n",
    "- Then, evaluate a more powerful ensemble model (Random Forest), and compare their performance. \n",
    "- Our goal will be to identify which approach offers the best balance between accuracy and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
